{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joojinho97/2022_GC_Covid19_prediction/blob/main/multi_input_one.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RjcpI7mgq9y",
        "outputId": "6784c06a-b73f-450e-e6b3-93201f3cc479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting medpy\n",
            "  Downloading MedPy-0.4.0.tar.gz (151 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▏                             | 10 kB 37.4 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20 kB 39.9 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 51 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 71 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 81 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 92 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 102 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 112 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 122 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 133 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 143 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 151 kB 14.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from medpy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from medpy) (1.19.5)\n",
            "Collecting SimpleITK>=1.1.0\n",
            "  Downloading SimpleITK-2.1.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (48.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 48.4 MB 1.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: medpy\n",
            "  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for medpy: filename=MedPy-0.4.0-cp37-cp37m-linux_x86_64.whl size=754460 sha256=f1b75dc156a7cab41587bc79d9a9c1c856a8f74c400ab3012caa71879e9e5eb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/57/3a/da1183f22a6afb42e11138daa6a759de233fd977a984333602\n",
            "Successfully built medpy\n",
            "Installing collected packages: SimpleITK, medpy\n",
            "Successfully installed SimpleITK-2.1.1 medpy-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install medpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laf7Lx84gsFH"
      },
      "outputs": [],
      "source": [
        "import medpy\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import SimpleITK as sitk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Dense,Flatten,Conv2D,Concatenate,MaxPooling2D,BatchNormalization,Activation,Add,Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import EfficientNetB7,EfficientNetB6,EfficientNetB5,EfficientNetB3,EfficientNetB1,ResNet101,ResNet50,ResNet152"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib"
      ],
      "metadata": {
        "id": "gzmqfc_LWjdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljuxZrcB7jXC"
      },
      "outputs": [],
      "source": [
        "from medpy.io import load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x5F4v0VVchc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/grand_challenge/destination/data/mha/')\n",
        "\n",
        "data_list_f=glob.glob('*.mha')\n",
        "data_list_f.sort(key=lambda fname: int(fname.split('.')[0]))\n",
        "\n",
        "\n",
        "\n",
        "data_list_3=glob.glob('*.mha')\n",
        "data_list_3.sort(key=lambda fname: int(fname.split('.')[0]))\n",
        "\n",
        "data_list_4=glob.glob('*.mha')\n",
        "data_list_4.sort(key=lambda fname: int(fname.split('.')[0]))\n",
        "\n",
        "for i in range(len(data_list_3)):\n",
        "  \n",
        "  \n",
        "  data_list_3[i]='/content/drive/MyDrive/grand_challenge/destination/data/test_0115/'+data_list_3[i]\n",
        "  data_list_4[i]='/content/drive/MyDrive/grand_challenge/destination/data/test_0111/'+data_list_4[i]\n",
        "  data_list_f[i]='/content/drive/MyDrive/grand_challenge/destination/data/mha/'+data_list_f[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDyzzS2H26sp"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv('/content/drive/MyDrive/grand_challenge/destination/metadata/reference.csv')\n",
        "data=data.sort_values(by='PatientID')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_ag=pd.read_csv('/content/drive/MyDrive/grand_challenge/destination/metadata/age_gender.csv')\n",
        "data_ag=list(zip(data_ag['age'].tolist(),data_ag['gender'].tolist()))\n"
      ],
      "metadata": {
        "id": "auN6rBiwvrcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_ag=data_ag[:1400]\n",
        "val_data_ag=data_ag[1401:]"
      ],
      "metadata": {
        "id": "-MfYOZ45v683"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueHeAkoebRSj"
      },
      "outputs": [],
      "source": [
        "tr=list(zip(data[\"probCOVID\"].tolist(), data[\"probSevere\"].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_list_f=data_list_f[:1400]\n",
        "val_list_f=data_list_f[1401:]\n"
      ],
      "metadata": {
        "id": "wE8hdAXy3R__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list_4=data_list_4[:1400]\n",
        "val_list_4=data_list_4[1401:]"
      ],
      "metadata": {
        "id": "PsvKuhBP-Esj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list_3=data_list_3[:1400]\n",
        "val_list_3=data_list_3[1401:]\n",
        "train_label=tr[:1400]\n",
        "val_label=tr[1401:]"
      ],
      "metadata": {
        "id": "B2-dS3N5PoFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre(img,img4):\n",
        "  \n",
        "  img  = cv2.normalize(np.float32(img),None,img.numpy().min(),img.numpy().max(),cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "  img4  = cv2.normalize(np.float32(img4),None,img4.numpy().min(),img4.numpy().max(),cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "  if int(len(img.shape)) == 2:\n",
        "    img=np.expand_dims(img,axis=-1)\n",
        "  if int(len(img4.shape)) == 2:\n",
        "    img4=np.expand_dims(img4,axis=-1)\n",
        "  return img,img4\n",
        "  \n",
        "def data_(dataset):\n",
        "  dataset=dataset.map(file_replace)\n",
        "  dataset=dataset.map(data_preprocess)  \n",
        "  return  dataset\n",
        "\n",
        "def file_replace(path,path4,data_ag,labels):\n",
        "\n",
        "  return tf.strings.regex_replace(path, '.mha', ''), tf.strings.regex_replace(path4, '.mha', '.jpg'),data_ag,labels\n",
        "\n",
        "def data_preprocess(x,path4, data_ag,labels):\n",
        "  \n",
        "  raw=tf.io.read_file(x)\n",
        "  img = tf.image.decode_png(raw, channels=4)\n",
        "\n",
        "  raw4=tf.io.read_file(path4)\n",
        "  img4 = tf.image.decode_png(raw4, channels=3)\n",
        "  \n",
        "  img,img4=tf.py_function(pre,inp=[img,img4],Tout=(tf.float32,tf.float32))\n",
        "  data_ag=tf.cast(data_ag,tf.int8)\n",
        "  \n",
        "  return {'input_1':img,'input_2':img4, 'age':data_ag[0],'gender':data_ag[1]}, {'probCOVID':labels[0], 'probSevere' :labels[1]}\n",
        "  "
      ],
      "metadata": {
        "id": "QXHwZg9NiWt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLgA16HI6w6N"
      },
      "outputs": [],
      "source": [
        "def make_dataset(train_list_3,train_list_4,data_ag, tr):\n",
        "  test_flist=tf.data.Dataset.from_tensor_slices((train_list_3,train_list_4,data_ag,tr))\n",
        "  test_dataset=data_(test_flist)\n",
        "  test_dataset=test_dataset.batch(2)\n",
        "  test_dataset=test_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  test_datset = test_dataset.apply(tf.data.experimental.ignore_errors())\n",
        "  return test_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Bdw6fMtD6xS"
      },
      "outputs": [],
      "source": [
        "train_dataset=make_dataset(train_list_3,train_list_4, train_data_ag,train_label)\n",
        "val_dataset=make_dataset(val_list_3,val_list_4,val_data_ag,val_label)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j in train_dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "g65AWyGAsMYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7x3G69-33xm0"
      },
      "outputs": [],
      "source": [
        "input_img=Input(shape=(512,512,4),name='input_1')\n",
        "input_shape=(512,512,4)\n",
        "input_img2=Input(shape=(256,256,3),name='input_2')\n",
        "input_shape2=(256,256,3)\n",
        "input_age=Input(shape=(1,),name='age')\n",
        "input_gender=Input(shape=(1,),name='gender')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G4pwFTckKIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02838f7c-cb19-4cd9-ff62-da7bb3bcb402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/applications/resnet.py:141: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 4 input channels.\n",
            "  weights=weights)\n"
          ]
        }
      ],
      "source": [
        "ef=ResNet50(weights=None,include_top=False, input_shape=input_shape, pooling=\"max\")(input_img)\n",
        "ef2=ResNet101(weights=None,include_top=False, input_shape=input_shape2, pooling=\"max\")(input_img2)\n",
        "\n",
        "\n",
        "\n",
        "age_dense = tf.keras.layers.Dense(1, activation='relu')(input_age)\n",
        "gender_dense = tf.keras.layers.Dense(1, activation='relu')(input_gender)\n",
        "\n",
        "covid_dense=Concatenate()([ef,ef2])\n",
        "covid_dense=BatchNormalization()(covid_dense)\n",
        "\n",
        "predictions_covid = tf.keras.layers.Dense(1, activation='sigmoid',name='probCOVID')(covid_dense)\n",
        "\n",
        "severe_dense=Concatenate()([ef,ef2,input_gender,input_age,predictions_covid])\n",
        "severe_dense=BatchNormalization()(severe_dense)\n",
        "#*5 빼고해보기\n",
        "#ef ef2 빼고 3개에다가 conv1d하나써서 해보는 방식\n",
        "\n",
        "predictions_severe = tf.keras.layers.Dense(1, activation='sigmoid',name='probSevere')(severe_dense)\n",
        "model = tf.keras.Model(inputs=[input_img,input_img2,input_age,input_gender], outputs=[predictions_covid,predictions_severe])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ket39Mz44FQj"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss={'probCOVID': 'binary_crossentropy', 'probSevere':'binary_crossentropy'},metrics=[tf.keras.metrics.AUC(multi_label=True, curve='ROC', name='auc_pr')])\n",
        "# loss_weight=[1,2] 이런식으로 가중치 부여도 해보기 ,loss_weights=[1,7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX2coLXQOtqJ"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/grand_challenge/destination/data/h5/resnet50/{epoch:02d}_A.h5', verbose=True, save_weights_only=True, mode='auto'),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_probCOVID_auc_pr', factor=0.8, patience=2, verbose=0, mode='max', min_delta=0.0001, cooldown=0, min_lr=0),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qw7thKv04boV"
      },
      "outputs": [],
      "source": [
        "model.fit(train_dataset, epochs=150,validation_data=val_dataset,callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = sitk.ReadImage('/content/drive/MyDrive/grand_challenge/destination/data/mha/6.mha')"
      ],
      "metadata": {
        "id": "tQiYvPm7M-jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k8mQTKZjVXYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "베스트 결과 모델 구조 +  val 결과\n"
      ],
      "metadata": {
        "id": "0TFrGx3_TyRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(len(data_list_3)):\n",
        "  \n",
        "  \n",
        "  data_list_3[i]='/content/drive/MyDrive/grand_challenge/destination/data/test_0115/'+data_list_3[i]\n",
        "  data_list_4[i]='/content/drive/MyDrive/grand_challenge/destination/data/test_0111/'+data_list_4[i]\n",
        "  data_list_f[i]='/content/drive/MyDrive/grand_challenge/destination/data/mha/'+data_list_f[i]"
      ],
      "metadata": {
        "id": "DwGr7bqUVcUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre(img,img4):\n",
        "  img  = cv2.normalize(np.float32(img),None,0,1,cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "  img4  = cv2.normalize(np.float32(img4),None,0,1,cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "  if int(len(img.shape)) == 2:\n",
        "    img=np.expand_dims(img,axis=-1)\n",
        "  if int(len(img4.shape)) == 2:\n",
        "    img4=np.expand_dims(img4,axis=-1)\n",
        "  return img,img4\n",
        "\n",
        "def data_(dataset):\n",
        "  dataset=dataset.map(file_replace)\n",
        "  dataset=dataset.map(data_preprocess)  \n",
        "  return  dataset\n",
        "\n",
        "def file_replace(path,path4,data_ag,labels):\n",
        "\n",
        "  return tf.strings.regex_replace(path, '.mha', ''), tf.strings.regex_replace(path4, '.mha', '.jpg'),data_ag,labels\n",
        "\n",
        "def data_preprocess(x,path4, data_ag,labels):\n",
        "  \n",
        "  raw=tf.io.read_file(x)\n",
        "  img = tf.image.decode_png(raw, channels=4)\n",
        "\n",
        "  raw4=tf.io.read_file(path4)\n",
        "  img4 = tf.image.decode_png(raw4, channels=3)\n",
        "  \n",
        "  img,img4=tf.py_function(pre,inp=[img,img4],Tout=(tf.float32,tf.float32))\n",
        "  data_ag=tf.cast(data_ag,tf.int8)\n",
        "  \n",
        "  return {'input_1':img,'input_2':img4, 'age':data_ag[0],'gender':data_ag[1]}, {'probCOVID':labels[0], 'probSevere' :labels[1]}\n",
        "  "
      ],
      "metadata": {
        "id": "GCPX22P_Vhga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_img=Input(shape=(512,512,4),name='input_1')\n",
        "input_shape=(512,512,4)\n",
        "input_img2=Input(shape=(512,512,3),name='input_2')\n",
        "input_shape2=(512,512,3)\n",
        "input_age=Input(shape=(1,),name='age')\n",
        "input_gender=Input(shape=(1,),name='gender')"
      ],
      "metadata": {
        "id": "osC49k_BVXaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ef=ResNet50(weights=None,include_top=False, input_shape=input_shape, pooling=\"max\")(input_img)\n",
        "ef2=ResNet101(weights=None,include_top=False, input_shape=input_shape2, pooling=\"max\")(input_img2)\n",
        "\n",
        "age_dense = tf.keras.layers.Dense(1, activation='relu')(input_age)\n",
        "gender_dense = tf.keras.layers.Dense(1, activation='relu')(input_gender)\n",
        "\n",
        "covid_dense=Concatenate()([ef,ef2])\n",
        "predictions_covid = tf.keras.layers.Dense(1, activation='sigmoid',name='probCOVID')(covid_dense)\n",
        "\n",
        "severe_dense=Concatenate()([ef,ef2,input_gender,input_age,predictions_covid])\n",
        "predictions_severe = tf.keras.layers.Dense(1, activation='sigmoid',name='probSevere')(severe_dense)\n",
        "\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_img,input_img2,input_age,input_gender], outputs=[predictions_covid,predictions_severe])"
      ],
      "metadata": {
        "id": "hyhNbEL1k0jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_probCOVID_auc_pr: 0.6844 - val_probSevere_auc_pr: 0.7562 => 0.7203 epoch21"
      ],
      "metadata": {
        "id": "DrouMVPnUf0e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "multi_input_one.ipynb의 사본",
      "provenance": [],
      "background_execution": "on",
      "mount_file_id": "1889M_OtZVFoPnylPhodOLSD0YrD23c-i",
      "authorship_tag": "ABX9TyOGV9kijU6NUzNrZbuL8IA2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}